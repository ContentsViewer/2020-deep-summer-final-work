{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlibをnotebookで描画するためのコマンド．\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 14 02:15:40 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 950M    Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   75C    P0    N/A /  N/A |    530MiB /  2004MiB |      1%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1010      G   /usr/lib/xorg/Xorg                 37MiB |\r\n",
      "|    0   N/A  N/A      1759      G   /usr/lib/xorg/Xorg                199MiB |\r\n",
      "|    0   N/A  N/A      1999      G   /usr/bin/gnome-shell              146MiB |\r\n",
      "|    0   N/A  N/A      3992      G   ...AAAAAAAAA= --shared-files       51MiB |\r\n",
      "|    0   N/A  N/A     19509      G   ...e/Steam/ubuntu12_32/steam       31MiB |\r\n",
      "|    0   N/A  N/A     19522      G   ./steamwebhelper                    1MiB |\r\n",
      "|    0   N/A  N/A     19535      G   ...AAA --log-file=/home/ken/       45MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0+cu101'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rex_gym.envs.gym.gallop_env import RexReactiveEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "reset\n",
      "reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01120414, -0.02117952,  0.00483729,  0.01796641, -0.07631158,\n",
       "       -0.91086726,  1.34631204,  0.0472513 , -0.93400384,  1.33731786,\n",
       "        0.01247665, -0.87609415,  1.33471505,  0.01371606, -0.9166647 ,\n",
       "        1.3086133 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = RexReactiveEnv(terrain_id='mounts', terrain_type='png', render=True)\n",
    "# env = RexReactiveEnv(terrain_id='hills', terrain_type='csv', render=True)\n",
    "env = RexReactiveEnv(terrain_id='plane', terrain_type='random', render=True)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "OUT OF TRAJECTORY!\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "\n",
    "# 終了シグナル(done=True)が返ってくるまで，ランダムに環境を動かす．\n",
    "while (not done):\n",
    "  action = env.action_space.sample()\n",
    "  _, _, done, _ = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rex_gym.envs.gym.walk_env import RexWalkEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "reset\n",
      "reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01120026, -0.02116809,  0.00481351,  0.0178849 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RexWalkEnv(terrain_id='plane', terrain_type='random', render=True)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cef93fedc19f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/envs/rex_gym_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_action_to_motor_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_termination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/model/rex.py\u001b[0m in \u001b[0;36mStep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pybullet_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepSimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReceiveObservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "\n",
    "# 終了シグナル(done=True)が返ってくるまで，ランダムに環境を動かす．\n",
    "while (not done):\n",
    "  action = env.action_space.sample()\n",
    "  _, _, done, _ = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, env, env_test, algo, seed=0, num_steps=10**6, eval_interval=10**4, num_eval_episodes=3):\n",
    "\n",
    "        self.env = env\n",
    "        self.env_test = env_test\n",
    "        self.algo = algo\n",
    "\n",
    "        # 環境の乱数シードを設定する．\n",
    "        self.env.seed(seed)\n",
    "        self.env_test.seed(2**31-seed)\n",
    "\n",
    "        # 平均収益を保存するための辞書．\n",
    "        self.returns = {'step': [], 'return': []}\n",
    "\n",
    "        # データ収集を行うステップ数．\n",
    "        self.num_steps = num_steps\n",
    "        # 評価の間のステップ数(インターバル)．\n",
    "        self.eval_interval = eval_interval\n",
    "        # 評価を行うエピソード数．\n",
    "        self.num_eval_episodes = num_eval_episodes\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" num_stepsステップの間，データ収集・学習・評価を繰り返す． \"\"\"\n",
    "\n",
    "        # 学習開始の時間\n",
    "        self.start_time = time()\n",
    "        # エピソードのステップ数．\n",
    "        t = 0\n",
    "\n",
    "        # 環境を初期化する．\n",
    "        state = self.env.reset()\n",
    "\n",
    "        for steps in range(1, self.num_steps + 1):\n",
    "            # 環境(self.env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            # アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\n",
    "            state, t = self.algo.step(self.env, state, t, steps)\n",
    "\n",
    "            # アルゴリズムが準備できていれば，1回学習を行う．\n",
    "            if self.algo.is_update(steps):\n",
    "                self.algo.update()\n",
    "\n",
    "            # 一定のインターバルで評価する．\n",
    "            if steps % self.eval_interval == 0:\n",
    "                self.evaluate(steps)\n",
    "\n",
    "    def evaluate(self, steps):\n",
    "        \"\"\" 複数エピソード環境を動かし，平均収益を記録する． \"\"\"\n",
    "\n",
    "        returns = []\n",
    "        for _ in range(self.num_eval_episodes):\n",
    "            state = self.env_test.reset()\n",
    "            done = False\n",
    "            episode_return = 0.0\n",
    "\n",
    "            while (not done):\n",
    "                action = self.algo.exploit(state)\n",
    "                state, reward, done, _ = self.env_test.step(action)\n",
    "                episode_return += reward\n",
    "\n",
    "            returns.append(episode_return)\n",
    "\n",
    "        mean_return = np.mean(returns)\n",
    "        self.returns['step'].append(steps)\n",
    "        self.returns['return'].append(mean_return)\n",
    "\n",
    "        print(f'Num steps: {steps:<6}   '\n",
    "              f'Return: {mean_return:<5.1f}   '\n",
    "              f'Time: {self.time}')\n",
    "\n",
    "    def visualize(self, env):\n",
    "        \"\"\" 1エピソード環境を動かし，mp4を再生する． \"\"\"\n",
    "#         env = wrap_monitor(gym.make(self.env.unwrapped.spec.id))\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while (not done):\n",
    "            action = self.algo.exploit(state)\n",
    "            state, _, done, _ = env.step(action)\n",
    "\n",
    "#         del env\n",
    "#         return play_mp4()\n",
    "        return\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\" 平均収益のグラフを描画する． \"\"\"\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.returns['step'], self.returns['return'])\n",
    "        plt.xlabel('Steps', fontsize=24)\n",
    "        plt.ylabel('Return', fontsize=24)\n",
    "        plt.tick_params(labelsize=18)\n",
    "#         plt.title(f'{self.env.unwrapped.spec.id}', fontsize=24)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        \"\"\" 学習開始からの経過時間． \"\"\"\n",
    "        return str(timedelta(seconds=int(time() - self.start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm(ABC):\n",
    "\n",
    "    def explore(self, state):\n",
    "        \"\"\" 確率論的な行動と，その行動の確率密度の対数 \\log(\\pi(a|s)) を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
    "        with torch.no_grad():\n",
    "            action, log_pi = self.actor.sample(state)\n",
    "        return action.cpu().numpy()[0], log_pi.item()\n",
    "\n",
    "    def exploit(self, state):\n",
    "        \"\"\" 決定論的な行動を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_update(self, steps):\n",
    "        \"\"\" 現在のトータルのステップ数(steps)を受け取り，アルゴリズムを学習するか否かを返す． \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, env, state, t, steps):\n",
    "        \"\"\" 環境(env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            受け取り，リプレイバッファへの保存などの処理を行い，状態・エピソードのステップ数を更新する．\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        \"\"\" 1回分の学習を行う． \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_pi(log_stds, noises, actions):\n",
    "    \"\"\" 確率論的な行動の確率密度を返す． \"\"\"\n",
    "\n",
    "    # NOTE: 入力はすべて (batch_size, |A|) となっているので，この関数では　batch_size　分の確率密度の対数 \\log \\pi(a|s) を\n",
    "    # それぞれ独立に計算し (batch_size, 1) で返します．\n",
    "\n",
    "    # ガウス分布 `N(0, stds * I)` における `noises * stds` の確率密度の対数(= \\log \\pi(u|a))を計算する．\n",
    "    stds = log_stds.exp()\n",
    "    gaussian_log_probs = Normal(torch.zeros_like(stds), stds).log_prob(stds * noises).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # NOTE: gaussian_log_probs には (batch_size, 1) で表された確率密度の対数 \\log p(u|s) が入っています．\n",
    "\n",
    "    # [演習] その後，tanh による確率密度の変化を修正しましょう．\n",
    "    # (例)\n",
    "    # log_pis = gaussian_log_probs - ...\n",
    "    log_pis = gaussian_log_probs - torch.log(1 - actions.pow(2) + 1e-6).sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    return log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(means, log_stds):\n",
    "    \"\"\" Reparameterization Trickを用いて，確率論的な行動とその確率密度を返す． \"\"\"\n",
    "\n",
    "    # 標準偏差．\n",
    "    stds = log_stds.exp()\n",
    "\n",
    "    # [演習] Reparameterization Trickを用いて，標準ガウス分布からノイズをサンプリングし，確率論的な行動を計算しましょう．\n",
    "    # (例)\n",
    "    # noises = ...\n",
    "    # actions = ...\n",
    "    noises = torch.randn_like(means)\n",
    "    us = means + noises * stds\n",
    "    actions = torch.tanh(us)\n",
    "\n",
    "    # 確率論的な行動の確率密度の対数を計算する．\n",
    "    log_pis = calculate_log_pi(log_stds, noises, actions)\n",
    "\n",
    "    return actions, log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACActor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        # 状態を受け取り，ガウス分布の平均と標準偏差の対数を出力するネットワークを構築します．\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 2 * action_shape[0]),\n",
    "        )\n",
    "\n",
    "    def forward(self, states):\n",
    "        # [演習] 決定論的な行動を計算し，返します．\n",
    "        # return ...\n",
    "        return torch.tanh(self.net(states).chunk(2, dim=-1)[0])\n",
    "\n",
    "    def sample(self, states):\n",
    "        # [演習] 確率論的な行動と確率密度の対数を計算し，返します．\n",
    "        # means, log_stds = ...\n",
    "        # log_stds.clamp_(-20, 2)\n",
    "        # return ...\n",
    "        means, log_stds = self.net(states).chunk(2, dim=-1)\n",
    "        return reparameterize(means, log_stds.clamp_(-20, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        # 状態を受け取り，ソフト状態行動価値を出力するネットワークを2つ構築します．\n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.net2 = nn.Sequential(\n",
    "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        # [演習] ソフト状態行動価値を2つ計算し，返します．\n",
    "        # return ...\n",
    "        x = torch.cat([states, actions], dim=-1)\n",
    "        return self.net1(x), self.net2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, buffer_size, state_shape, action_shape, device):\n",
    "        # 次にデータを挿入するインデックス．\n",
    "        self._p = 0\n",
    "        # データ数．\n",
    "        self._n = 0\n",
    "        # リプレイバッファのサイズ．\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        # GPU上に保存するデータ．\n",
    "        self.states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "        self.actions = torch.empty((buffer_size, *action_shape), dtype=torch.float, device=device)\n",
    "        self.rewards = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.dones = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.next_states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.states[self._p].copy_(torch.from_numpy(state))\n",
    "        self.actions[self._p].copy_(torch.from_numpy(action))\n",
    "        self.rewards[self._p] = float(reward)\n",
    "        self.dones[self._p] = float(done)\n",
    "        self.next_states[self._p].copy_(torch.from_numpy(next_state))\n",
    "\n",
    "        self._p = (self._p + 1) % self.buffer_size\n",
    "        self._n = min(self._n + 1, self.buffer_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxes = np.random.randint(low=0, high=self._n, size=batch_size)\n",
    "        return (\n",
    "            self.states[idxes],\n",
    "            self.actions[idxes],\n",
    "            self.rewards[idxes],\n",
    "            self.dones[idxes],\n",
    "            self.next_states[idxes]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(Algorithm):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape, device=torch.device('cuda'), seed=0,\n",
    "                 batch_size=256, gamma=0.99, lr_actor=3e-4, lr_critic=3e-4,\n",
    "                 replay_size=10**6, start_steps=10**4, tau=5e-3, alpha=0.2, reward_scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # シードを設定する．\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # リプレイバッファ．\n",
    "        self.buffer = ReplayBuffer(\n",
    "            buffer_size=replay_size,\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # Actor-Criticのネットワークを構築する．\n",
    "        self.actor = SACActor(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device)\n",
    "        self.critic = SACCritic(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device)\n",
    "        self.critic_target = SACCritic(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device).eval()\n",
    "\n",
    "        # ターゲットネットワークの重みを初期化し，勾配計算を無効にする．\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        for param in self.critic_target.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # オプティマイザ．\n",
    "        self.optim_actor = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.optim_critic = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        # その他パラメータ．\n",
    "        self.learning_steps = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.start_steps = start_steps\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.reward_scale = reward_scale\n",
    "\n",
    "    def is_update(self, steps):\n",
    "        # 学習初期の一定期間(start_steps)は学習しない．\n",
    "        return steps >= max(self.start_steps, self.batch_size)\n",
    "\n",
    "    def step(self, env, state, t, steps):\n",
    "        t += 1\n",
    "\n",
    "        # 学習初期の一定期間(start_steps)は，ランダムに行動して多様なデータの収集を促進する．\n",
    "        if steps <= self.start_steps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action, _ = self.explore(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # ゲームオーバーではなく，最大ステップ数に到達したことでエピソードが終了した場合は，\n",
    "        # 本来であればその先もMDPが継続するはず．よって，終了シグナルをFalseにする．\n",
    "        if t == 200:\n",
    "            done_masked = False\n",
    "            done=True\n",
    "        else:\n",
    "            done_masked = done\n",
    "        \n",
    "\n",
    "        # リプレイバッファにデータを追加する．\n",
    "        self.buffer.append(state, action, reward, done_masked, next_state)\n",
    "\n",
    "        # エピソードが終了した場合には，環境をリセットする．\n",
    "        if done:\n",
    "            t = 0\n",
    "            next_state = env.reset()\n",
    "\n",
    "        return next_state, t\n",
    "\n",
    "    def update(self):\n",
    "        self.learning_steps += 1\n",
    "        states, actions, rewards, dones, next_states = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        self.update_critic(states, actions, rewards, dones, next_states)\n",
    "        self.update_actor(states)\n",
    "        self.update_target()\n",
    "\n",
    "    def update_critic(self, states, actions, rewards, dones, next_states):\n",
    "        # [演習] 現在のソフト状態行動価値とそのターゲットを計算し，ネットワークの損失関数を完成させましょう．\n",
    "        # (例)\n",
    "        # 現在のソフト状態行動価値を計算する．\n",
    "        # curr_qs1, curr_qs2 = ...\n",
    "        curr_qs1, curr_qs2 = self.critic(states, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "              # ソフト状態価値のターゲットを計算します．\n",
    "              # target_vs = ...\n",
    "            next_actions, log_pis = self.actor.sample(next_states)\n",
    "            next_qs1, next_qs2 = self.critic_target(next_states, next_actions)\n",
    "            next_qs = torch.min(next_qs1, next_qs2) - self.alpha * log_pis\n",
    "\n",
    "        # ソフト状態行動価値のターゲットを計算します．\n",
    "        # target_qs = ...\n",
    "        target_qs = rewards * self.reward_scale + (1.0 - dones) * self.gamma * next_qs\n",
    "\n",
    "        loss_critic1 = (curr_qs1 - target_qs).pow_(2).mean()\n",
    "        loss_critic2 = (curr_qs2 - target_qs).pow_(2).mean()\n",
    "\n",
    "        self.optim_critic.zero_grad()\n",
    "        (loss_critic1 + loss_critic2).backward(retain_graph=False)\n",
    "        self.optim_critic.step()\n",
    "\n",
    "    def update_actor(self, states):\n",
    "        # [演習] 方策のネットワークの損失関数を計算しましょう．\n",
    "        # (例)\n",
    "        # loss_actor = ... self.actor ...\n",
    "        actions, log_pis = self.actor.sample(states)\n",
    "        qs1, qs2 = self.critic(states, actions)\n",
    "        loss_actor = (self.alpha * log_pis - torch.min(qs1, qs2)).mean()\n",
    "\n",
    "        self.optim_actor.zero_grad()\n",
    "        loss_actor.backward(retain_graph=False)\n",
    "        self.optim_actor.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        for t, s in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            t.data.mul_(1.0 - self.tau)\n",
    "            t.data.add_(self.tau * s.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "reset\n",
      "reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SEED = 0\n",
    "REWARD_SCALE = 50.0\n",
    "\n",
    "NUM_STEPS = 10 ** 6\n",
    "EVAL_INTERVAL = 10 ** 4\n",
    "\n",
    "env = RexReactiveEnv(terrain_id='plane', terrain_type='random', render=False)\n",
    "env_test = RexReactiveEnv(terrain_id='plane', terrain_type='random', render=False)\n",
    "\n",
    "algo = SAC(\n",
    "    state_shape=env.observation_space.shape,\n",
    "    action_shape=env.action_space.shape,\n",
    "    seed=SEED,\n",
    "    reward_scale=REWARD_SCALE\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    env=env,\n",
    "    env_test=env_test,\n",
    "    algo=algo,\n",
    "    seed=SEED,\n",
    "    num_steps=NUM_STEPS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "Num steps: 10000    Return: -21.5   Time: 0:02:55\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "OUT OF TRAJECTORY!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "Num steps: 20000    Return: -10.5   Time: 0:07:11\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "Num steps: 30000    Return: -10.7   Time: 0:11:14\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "OUT OF TRAJECTORY!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "Num steps: 40000    Return: -10.9   Time: 0:15:22\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "OUT OF TRAJECTORY!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "OUT OF TRAJECTORY!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "OUT OF TRAJECTORY!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "Num steps: 50000    Return: -10.6   Time: 0:19:45\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "Num steps: 60000    Return: -10.2   Time: 0:23:56\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "OUT OF TRAJECTORY!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "Num steps: 70000    Return: -10.4   Time: 0:28:15\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "OUT OF TRAJECTORY!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "Num steps: 80000    Return: -10.8   Time: 0:32:35\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "reset\n",
      "reset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-17e3ffbe9350>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# 環境(self.env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# アルゴリズムが準備できていれば，1回学習を行う．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-517b64146130>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, env, state, t, steps)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# ゲームオーバーではなく，最大ステップ数に到達したことでエピソードが終了した場合は，\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/envs/rex_gym_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0menv_randomizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomize_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_action_to_motor_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/envs/gym/gallop_env.py\u001b[0m in \u001b[0;36m_transform_action_to_motor_command\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetTimeSinceReset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_target_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/envs/gym/gallop_env.py\u001b[0m in \u001b[0;36m_signal\u001b[0;34m(self, t, action)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ik'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IK_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ol'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_loop_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/envs/gym/gallop_env.py\u001b[0m in \u001b[0;36m_IK_signal\u001b[0;34m(self, t, action)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mbrakes_coeff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate_brakes_stage_coeff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mstep_length\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mbrakes_coeff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gait_planner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_angle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_rotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0mfr_angles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfl_angles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrr_angles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl_angles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kinematics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         signal = [\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/model/gait_planner.py\u001b[0m in \u001b[0;36mloop\u001b[0;34m(self, v, angle, w_rot, t, direction, frames)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         step_coord = self.step_trajectory(self._phi + self._offset[1], v, angle, w_rot,\n\u001b[0;32m--> 118\u001b[0;31m                                           np.squeeze(np.asarray(frames[1, :])), direction)  # FL\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstep_coord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstep_coord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/model/gait_planner.py\u001b[0m in \u001b[0;36mstep_trajectory\u001b[0;34m(self, phi, v, angle, w_rot, center_to_foot, direction)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mphiSwing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_offset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mstepX_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepY_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepZ_long\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_bezier_swing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphiSwing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mstepX_rot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepY_rot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepZ_rot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_bezier_swing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphiSwing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_rot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcircle_trajectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcenter_to_foot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstepX_rot\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/model/gait_planner.py\u001b[0m in \u001b[0;36mcalculate_bezier_swing\u001b[0;34m(self, phi_sw, v, angle, direction)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mswing_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswing_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbezier_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi_sw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mswing_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswing_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbezier_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi_sw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mswing_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswing_z\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbezier_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi_sw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mswing_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswing_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswing_z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/model/gait_planner.py\u001b[0m in \u001b[0;36mbezier_curve\u001b[0;34m(self, t, k, point)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbezier_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_bin_factor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "Only one local in-process GUI/GUI_SERVER connection allowed. Use DIRECT connection mode or start a separate GUI physics server (ExampleBrowser, App_SharedMemoryPhysics_GUI, App_SharedMemoryPhysics_VR) and connect over SHARED_MEMORY, UDP or TCP instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-77af707f5a8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv_visualize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRexReactiveEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterrain_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'plane'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterrain_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/envs/gym/gallop_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, debug, urdf_version, energy_weight, control_time_step, action_repeat, control_latency, pd_latency, on_rack, motor_kp, motor_kd, render, num_steps_to_log, use_angle_in_observation, env_randomizer, log_path, target_position, signal_type, terrain_type, terrain_id)\u001b[0m\n\u001b[1;32m    111\u001b[0m                              \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                              \u001b[0mterrain_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mterrain_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                              terrain_type=terrain_type)\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;31m# (eventually) allow different feedback ranges/action spaces for different signals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         action_max = {\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/envs/rex_gym_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, debug, urdf_root, urdf_version, distance_weight, energy_weight, shake_weight, drift_weight, distance_limit, observation_noise_stdev, self_collision_enabled, motor_velocity_limit, pd_control_enabled, leg_model_enabled, accurate_motor_model_enabled, remove_default_joint_damping, motor_kp, motor_kd, control_latency, pd_latency, torque_control_enabled, motor_overheat_protection, hard_reset, on_rack, render, num_steps_to_log, action_repeat, control_time_step, env_randomizer, forward_reward_cap, reflection, log_path, target_orient, init_orient, target_position, start_position, base_y, base_z, base_roll, base_pitch, base_yaw, step_length, step_rotation, step_angle, step_period, backwards, signal_type, terrain_type, terrain_id)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_render\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pybullet_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbullet_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBulletClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpybullet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGUI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pybullet_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbullet_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBulletClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/util/bullet_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, connection_mode)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mconnection_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpybullet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDIRECT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpybullet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: Only one local in-process GUI/GUI_SERVER connection allowed. Use DIRECT connection mode or start a separate GUI physics server (ExampleBrowser, App_SharedMemoryPhysics_GUI, App_SharedMemoryPhysics_VR) and connect over SHARED_MEMORY, UDP or TCP instead."
     ]
    }
   ],
   "source": [
    "\n",
    "env_visualize = RexReactiveEnv(terrain_id='plane', terrain_type='random', render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n"
     ]
    }
   ],
   "source": [
    "trainer.visualize(env_visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "del env_test\n",
    "del env_visualize\n",
    "del algo\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 0\n",
    "REWARD_SCALE = 50.0\n",
    "\n",
    "NUM_STEPS = 10 ** 6\n",
    "EVAL_INTERVAL = 10 ** 4\n",
    "\n",
    "env = RexWalkEnv(terrain_id='plane', terrain_type='random', render=False)\n",
    "env_test = RexWalkEnv(terrain_id='plane', terrain_type='random', render=False)\n",
    "\n",
    "algo = SAC(\n",
    "    state_shape=env.observation_space.shape,\n",
    "    action_shape=env.action_space.shape,\n",
    "    seed=SEED,\n",
    "    reward_scale=REWARD_SCALE\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    env=env,\n",
    "    env_test=env_test,\n",
    "    algo=algo,\n",
    "    seed=SEED,\n",
    "    num_steps=NUM_STEPS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_visualize = RexReactiveEnv(terrain_id='plane', terrain_type='random', render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.visualize(env_visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "del env_test\n",
    "del env_visualize\n",
    "del algo\n",
    "del trainer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
