{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlibをnotebookで描画するためのコマンド．\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 13 20:48:27 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 950M    Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   62C    P8    N/A /  N/A |    322MiB /  2004MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1010      G   /usr/lib/xorg/Xorg                 37MiB |\r\n",
      "|    0   N/A  N/A      1759      G   /usr/lib/xorg/Xorg                140MiB |\r\n",
      "|    0   N/A  N/A      1999      G   /usr/bin/gnome-shell               94MiB |\r\n",
      "|    0   N/A  N/A      3992      G   ...AAAAAAAAA= --shared-files       36MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0+cu101'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rex_gym.envs.gym.gallop_env import RexReactiveEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "reset\n"
     ]
    }
   ],
   "source": [
    "env = RexReactiveEnv(terrain_id='mounts', terrain_type='png', render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4.00468651e-06, -1.01224217e-02, -3.80417328e-05, -1.38148780e-02,\n",
       "       -2.68621739e-06, -9.17017298e-01,  1.34539809e+00,  1.43645563e-06,\n",
       "       -9.17004719e-01,  1.34539665e+00,  1.38581056e-05, -9.01149105e-01,\n",
       "        1.31925961e+00,  1.02100967e-05, -9.01124119e-01,  1.31923738e+00])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "FALLING DOWN!\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "\n",
    "# 終了シグナル(done=True)が返ってくるまで，ランダムに環境を動かす．\n",
    "while (not done):\n",
    "  action = env.action_space.sample()\n",
    "  _, _, done, _ = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, env, env_test, algo, seed=0, num_steps=10**6, eval_interval=10**4, num_eval_episodes=3):\n",
    "\n",
    "        self.env = env\n",
    "        self.env_test = env_test\n",
    "        self.algo = algo\n",
    "\n",
    "        # 環境の乱数シードを設定する．\n",
    "        self.env.seed(seed)\n",
    "        self.env_test.seed(2**31-seed)\n",
    "\n",
    "        # 平均収益を保存するための辞書．\n",
    "        self.returns = {'step': [], 'return': []}\n",
    "\n",
    "        # データ収集を行うステップ数．\n",
    "        self.num_steps = num_steps\n",
    "        # 評価の間のステップ数(インターバル)．\n",
    "        self.eval_interval = eval_interval\n",
    "        # 評価を行うエピソード数．\n",
    "        self.num_eval_episodes = num_eval_episodes\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" num_stepsステップの間，データ収集・学習・評価を繰り返す． \"\"\"\n",
    "\n",
    "        # 学習開始の時間\n",
    "        self.start_time = time()\n",
    "        # エピソードのステップ数．\n",
    "        t = 0\n",
    "\n",
    "        # 環境を初期化する．\n",
    "        state = self.env.reset()\n",
    "\n",
    "        for steps in range(1, self.num_steps + 1):\n",
    "            # 環境(self.env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            # アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\n",
    "            state, t = self.algo.step(self.env, state, t, steps)\n",
    "\n",
    "            # アルゴリズムが準備できていれば，1回学習を行う．\n",
    "            if self.algo.is_update(steps):\n",
    "                self.algo.update()\n",
    "\n",
    "            # 一定のインターバルで評価する．\n",
    "            if steps % self.eval_interval == 0:\n",
    "                self.evaluate(steps)\n",
    "\n",
    "    def evaluate(self, steps):\n",
    "        \"\"\" 複数エピソード環境を動かし，平均収益を記録する． \"\"\"\n",
    "\n",
    "        returns = []\n",
    "        for _ in range(self.num_eval_episodes):\n",
    "            state = self.env_test.reset()\n",
    "            done = False\n",
    "            episode_return = 0.0\n",
    "\n",
    "            while (not done):\n",
    "                action = self.algo.exploit(state)\n",
    "                state, reward, done, _ = self.env_test.step(action)\n",
    "                episode_return += reward\n",
    "\n",
    "            returns.append(episode_return)\n",
    "\n",
    "        mean_return = np.mean(returns)\n",
    "        self.returns['step'].append(steps)\n",
    "        self.returns['return'].append(mean_return)\n",
    "\n",
    "        print(f'Num steps: {steps:<6}   '\n",
    "              f'Return: {mean_return:<5.1f}   '\n",
    "              f'Time: {self.time}')\n",
    "\n",
    "    def visualize(self, env):\n",
    "        \"\"\" 1エピソード環境を動かし，mp4を再生する． \"\"\"\n",
    "#         env = wrap_monitor(gym.make(self.env.unwrapped.spec.id))\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while (not done):\n",
    "            action = self.algo.exploit(state)\n",
    "            state, _, done, _ = env.step(action)\n",
    "\n",
    "#         del env\n",
    "#         return play_mp4()\n",
    "        return\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\" 平均収益のグラフを描画する． \"\"\"\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.returns['step'], self.returns['return'])\n",
    "        plt.xlabel('Steps', fontsize=24)\n",
    "        plt.ylabel('Return', fontsize=24)\n",
    "        plt.tick_params(labelsize=18)\n",
    "#         plt.title(f'{self.env.unwrapped.spec.id}', fontsize=24)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        \"\"\" 学習開始からの経過時間． \"\"\"\n",
    "        return str(timedelta(seconds=int(time() - self.start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm(ABC):\n",
    "\n",
    "    def explore(self, state):\n",
    "        \"\"\" 確率論的な行動と，その行動の確率密度の対数 \\log(\\pi(a|s)) を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
    "        with torch.no_grad():\n",
    "            action, log_pi = self.actor.sample(state)\n",
    "        return action.cpu().numpy()[0], log_pi.item()\n",
    "\n",
    "    def exploit(self, state):\n",
    "        \"\"\" 決定論的な行動を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_update(self, steps):\n",
    "        \"\"\" 現在のトータルのステップ数(steps)を受け取り，アルゴリズムを学習するか否かを返す． \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, env, state, t, steps):\n",
    "        \"\"\" 環境(env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            受け取り，リプレイバッファへの保存などの処理を行い，状態・エピソードのステップ数を更新する．\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        \"\"\" 1回分の学習を行う． \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_pi(log_stds, noises, actions):\n",
    "    \"\"\" 確率論的な行動の確率密度を返す． \"\"\"\n",
    "\n",
    "    # NOTE: 入力はすべて (batch_size, |A|) となっているので，この関数では　batch_size　分の確率密度の対数 \\log \\pi(a|s) を\n",
    "    # それぞれ独立に計算し (batch_size, 1) で返します．\n",
    "\n",
    "    # ガウス分布 `N(0, stds * I)` における `noises * stds` の確率密度の対数(= \\log \\pi(u|a))を計算する．\n",
    "    stds = log_stds.exp()\n",
    "    gaussian_log_probs = Normal(torch.zeros_like(stds), stds).log_prob(stds * noises).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # NOTE: gaussian_log_probs には (batch_size, 1) で表された確率密度の対数 \\log p(u|s) が入っています．\n",
    "\n",
    "    # [演習] その後，tanh による確率密度の変化を修正しましょう．\n",
    "    # (例)\n",
    "    # log_pis = gaussian_log_probs - ...\n",
    "    log_pis = gaussian_log_probs - torch.log(1 - actions.pow(2) + 1e-6).sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    return log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(means, log_stds):\n",
    "    \"\"\" Reparameterization Trickを用いて，確率論的な行動とその確率密度を返す． \"\"\"\n",
    "\n",
    "    # 標準偏差．\n",
    "    stds = log_stds.exp()\n",
    "\n",
    "    # [演習] Reparameterization Trickを用いて，標準ガウス分布からノイズをサンプリングし，確率論的な行動を計算しましょう．\n",
    "    # (例)\n",
    "    # noises = ...\n",
    "    # actions = ...\n",
    "    noises = torch.randn_like(means)\n",
    "    us = means + noises * stds\n",
    "    actions = torch.tanh(us)\n",
    "\n",
    "    # 確率論的な行動の確率密度の対数を計算する．\n",
    "    log_pis = calculate_log_pi(log_stds, noises, actions)\n",
    "\n",
    "    return actions, log_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACActor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        # 状態を受け取り，ガウス分布の平均と標準偏差の対数を出力するネットワークを構築します．\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 2 * action_shape[0]),\n",
    "        )\n",
    "\n",
    "    def forward(self, states):\n",
    "        # [演習] 決定論的な行動を計算し，返します．\n",
    "        # return ...\n",
    "        return torch.tanh(self.net(states).chunk(2, dim=-1)[0])\n",
    "\n",
    "    def sample(self, states):\n",
    "        # [演習] 確率論的な行動と確率密度の対数を計算し，返します．\n",
    "        # means, log_stds = ...\n",
    "        # log_stds.clamp_(-20, 2)\n",
    "        # return ...\n",
    "        means, log_stds = self.net(states).chunk(2, dim=-1)\n",
    "        return reparameterize(means, log_stds.clamp_(-20, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        # 状態を受け取り，ソフト状態行動価値を出力するネットワークを2つ構築します．\n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.net2 = nn.Sequential(\n",
    "            nn.Linear(state_shape[0] + action_shape[0], 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        # [演習] ソフト状態行動価値を2つ計算し，返します．\n",
    "        # return ...\n",
    "        x = torch.cat([states, actions], dim=-1)\n",
    "        return self.net1(x), self.net2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, buffer_size, state_shape, action_shape, device):\n",
    "        # 次にデータを挿入するインデックス．\n",
    "        self._p = 0\n",
    "        # データ数．\n",
    "        self._n = 0\n",
    "        # リプレイバッファのサイズ．\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        # GPU上に保存するデータ．\n",
    "        self.states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "        self.actions = torch.empty((buffer_size, *action_shape), dtype=torch.float, device=device)\n",
    "        self.rewards = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.dones = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.next_states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.states[self._p].copy_(torch.from_numpy(state))\n",
    "        self.actions[self._p].copy_(torch.from_numpy(action))\n",
    "        self.rewards[self._p] = float(reward)\n",
    "        self.dones[self._p] = float(done)\n",
    "        self.next_states[self._p].copy_(torch.from_numpy(next_state))\n",
    "\n",
    "        self._p = (self._p + 1) % self.buffer_size\n",
    "        self._n = min(self._n + 1, self.buffer_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxes = np.random.randint(low=0, high=self._n, size=batch_size)\n",
    "        return (\n",
    "            self.states[idxes],\n",
    "            self.actions[idxes],\n",
    "            self.rewards[idxes],\n",
    "            self.dones[idxes],\n",
    "            self.next_states[idxes]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(Algorithm):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape, device=torch.device('cuda'), seed=0,\n",
    "                 batch_size=256, gamma=0.99, lr_actor=3e-4, lr_critic=3e-4,\n",
    "                 replay_size=10**6, start_steps=10**4, tau=5e-3, alpha=0.2, reward_scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # シードを設定する．\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # リプレイバッファ．\n",
    "        self.buffer = ReplayBuffer(\n",
    "            buffer_size=replay_size,\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # Actor-Criticのネットワークを構築する．\n",
    "        self.actor = SACActor(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device)\n",
    "        self.critic = SACCritic(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device)\n",
    "        self.critic_target = SACCritic(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape\n",
    "        ).to(device).eval()\n",
    "\n",
    "        # ターゲットネットワークの重みを初期化し，勾配計算を無効にする．\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        for param in self.critic_target.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # オプティマイザ．\n",
    "        self.optim_actor = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.optim_critic = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        # その他パラメータ．\n",
    "        self.learning_steps = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.start_steps = start_steps\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.reward_scale = reward_scale\n",
    "\n",
    "    def is_update(self, steps):\n",
    "        # 学習初期の一定期間(start_steps)は学習しない．\n",
    "        return steps >= max(self.start_steps, self.batch_size)\n",
    "\n",
    "    def step(self, env, state, t, steps):\n",
    "        t += 1\n",
    "\n",
    "        # 学習初期の一定期間(start_steps)は，ランダムに行動して多様なデータの収集を促進する．\n",
    "        if steps <= self.start_steps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action, _ = self.explore(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # ゲームオーバーではなく，最大ステップ数に到達したことでエピソードが終了した場合は，\n",
    "        # 本来であればその先もMDPが継続するはず．よって，終了シグナルをFalseにする．\n",
    "#         if t == env._max_episode_steps:\n",
    "#             done_masked = False\n",
    "#         else:\n",
    "#             done_masked = done\n",
    "        \n",
    "        done_masked = done\n",
    "\n",
    "        # リプレイバッファにデータを追加する．\n",
    "        self.buffer.append(state, action, reward, done_masked, next_state)\n",
    "\n",
    "        # エピソードが終了した場合には，環境をリセットする．\n",
    "        if done:\n",
    "            t = 0\n",
    "            next_state = env.reset()\n",
    "\n",
    "        return next_state, t\n",
    "\n",
    "    def update(self):\n",
    "        self.learning_steps += 1\n",
    "        states, actions, rewards, dones, next_states = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        self.update_critic(states, actions, rewards, dones, next_states)\n",
    "        self.update_actor(states)\n",
    "        self.update_target()\n",
    "\n",
    "    def update_critic(self, states, actions, rewards, dones, next_states):\n",
    "        # [演習] 現在のソフト状態行動価値とそのターゲットを計算し，ネットワークの損失関数を完成させましょう．\n",
    "        # (例)\n",
    "        # 現在のソフト状態行動価値を計算する．\n",
    "        # curr_qs1, curr_qs2 = ...\n",
    "        curr_qs1, curr_qs2 = self.critic(states, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "              # ソフト状態価値のターゲットを計算します．\n",
    "              # target_vs = ...\n",
    "            next_actions, log_pis = self.actor.sample(next_states)\n",
    "            next_qs1, next_qs2 = self.critic_target(next_states, next_actions)\n",
    "            next_qs = torch.min(next_qs1, next_qs2) - self.alpha * log_pis\n",
    "\n",
    "        # ソフト状態行動価値のターゲットを計算します．\n",
    "        # target_qs = ...\n",
    "        target_qs = rewards * self.reward_scale + (1.0 - dones) * self.gamma * next_qs\n",
    "\n",
    "        loss_critic1 = (curr_qs1 - target_qs).pow_(2).mean()\n",
    "        loss_critic2 = (curr_qs2 - target_qs).pow_(2).mean()\n",
    "\n",
    "        self.optim_critic.zero_grad()\n",
    "        (loss_critic1 + loss_critic2).backward(retain_graph=False)\n",
    "        self.optim_critic.step()\n",
    "\n",
    "    def update_actor(self, states):\n",
    "        # [演習] 方策のネットワークの損失関数を計算しましょう．\n",
    "        # (例)\n",
    "        # loss_actor = ... self.actor ...\n",
    "        actions, log_pis = self.actor.sample(states)\n",
    "        qs1, qs2 = self.critic(states, actions)\n",
    "        loss_actor = (self.alpha * log_pis - torch.min(qs1, qs2)).mean()\n",
    "\n",
    "        self.optim_actor.zero_grad()\n",
    "        loss_actor.backward(retain_graph=False)\n",
    "        self.optim_actor.step()\n",
    "\n",
    "    def update_target(self):\n",
    "        for t, s in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            t.data.mul_(1.0 - self.tau)\n",
    "            t.data.add_(self.tau * s.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "reset\n",
      "reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SEED = 0\n",
    "REWARD_SCALE = 5.0\n",
    "\n",
    "NUM_STEPS = 10 ** 6\n",
    "EVAL_INTERVAL = 10 ** 4\n",
    "\n",
    "env = RexReactiveEnv(terrain_id='mounts', terrain_type='png', render=False)\n",
    "env_test = RexReactiveEnv(terrain_id='mounts', terrain_type='png', render=False)\n",
    "\n",
    "algo = SAC(\n",
    "    state_shape=env.observation_space.shape,\n",
    "    action_shape=env.action_space.shape,\n",
    "    seed=SEED,\n",
    "    reward_scale=REWARD_SCALE\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    env=env,\n",
    "    env_test=env_test,\n",
    "    algo=algo,\n",
    "    seed=SEED,\n",
    "    num_steps=NUM_STEPS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n",
      "FALLING DOWN!\n",
      "reset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-b8d0c2c98b12>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# 環境(self.env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# アルゴリズムが準備できていれば，1回学習を行う．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-8c869080cab3>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, env, state, t, steps)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# ゲームオーバーではなく，最大ステップ数に到達したことでエピソードが終了した場合は，\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/envs/rex_gym_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_action_to_motor_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_termination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/model/rex.py\u001b[0m in \u001b[0;36mStep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pybullet_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepSimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReceiveObservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/model/rex.py\u001b[0m in \u001b[0;36mApplyAction\u001b[0;34m(self, motor_commands, motor_kps, motor_kds)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accurate_motor_model_enabled\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pd_control_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_GetPDObservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mqdot_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetTrueMotorVelocities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accurate_motor_model_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 actual_torque, observed_torque = self._motor_model.convert_to_torque(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/env-KSxiTnGN/lib/python3.7/site-packages/rex_gym/model/rex.py\u001b[0m in \u001b[0;36mGetTrueMotorVelocities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m         motor_velocities = [\n\u001b[1;32m    495\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pybullet_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetJointState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquadruped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotor_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmotor_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_motor_id_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         ]\n\u001b[1;32m    498\u001b[0m         \u001b[0mmotor_velocities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotor_velocities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_motor_direction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "reset\n",
      "reset\n",
      "FALLING DOWN!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_visualize = RexReactiveEnv(terrain_id='mounts', terrain_type='png', render=True)\n",
    "trainer.visualize(env_visualize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_visualize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b0565c9d9a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0menv_visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'env_visualize' is not defined"
     ]
    }
   ],
   "source": [
    "del env_visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "del env_test\n",
    "del algo\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
